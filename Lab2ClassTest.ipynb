{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp import *\n",
    "from utils import print_table\n",
    "\n",
    "REWARD = -0.4\n",
    "\n",
    "environment1 = GridMDP([[REWARD, REWARD, REWARD, REWARD],\n",
    "               [REWARD, None, REWARD, REWARD],\n",
    "               [REWARD, REWARD, REWARD, REWARD],\n",
    "               [REWARD, -1, REWARD, 10]],\n",
    "               terminals=[(1,0),(3,0)])\n",
    "\n",
    "REWARD1 = -20\n",
    "environment2 = GridMDP([[REWARD1, REWARD1, REWARD1, REWARD1],\n",
    "               [REWARD1, None, REWARD1, REWARD1],\n",
    "               [REWARD1, REWARD1, REWARD1, REWARD1],\n",
    "               [REWARD1, -1, REWARD1, 10]],\n",
    "               terminals=[(1,0),(3,0)])\n",
    "\n",
    "REWARD2 = 0\n",
    "environment3 = GridMDP([[REWARD2, REWARD2, REWARD2, REWARD2],\n",
    "               [REWARD2, None, REWARD2, REWARD2],\n",
    "               [REWARD2, REWARD2, REWARD2, REWARD2],\n",
    "               [REWARD2, -1, REWARD2, 10]],\n",
    "               terminals=[(1,0),(3,0)])\n",
    "\n",
    "REWARD3 = -2\n",
    "environment4 = GridMDP([[REWARD3, REWARD3, REWARD3, REWARD3],\n",
    "               [REWARD3, None, REWARD3, REWARD3],\n",
    "               [REWARD3, REWARD3, REWARD3, REWARD3],\n",
    "               [REWARD3, -1, REWARD3, 10]],\n",
    "               terminals=[(1,0),(3,0)])\n",
    "\n",
    "pi1 = best_policy(environment1, value_iteration(environment1, 0.001))\n",
    "print_table(environment1.to_arrows(pi1))\n",
    "print('\\n')\n",
    "pi2 = best_policy(environment2, value_iteration(environment2, 0.001))\n",
    "print_table(environment2.to_arrows(pi2))\n",
    "print('\\n')\n",
    "pi3 = best_policy(environment3, value_iteration(environment3, 0.001))\n",
    "print_table(environment3.to_arrows(pi3))\n",
    "print('\\n')\n",
    "pi4 = best_policy(environment4, value_iteration(environment4, 0.001))\n",
    "print_table(environment4.to_arrows(pi4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDPVacuumAgent:\n",
    "    def __init__(self, environment, policy):\n",
    "        self.environment = environment\n",
    "        self.policy = policy\n",
    "        self.start_state = (3,3)\n",
    "        self.current_state = self.start_state\n",
    "        self.path = [self.current_state]  # To log the path followed\n",
    "        self.reward = 0\n",
    "\n",
    "    def transition(self, state, direction): # applies direction to current_index\n",
    "        return [state[0] + direction[0], state[1] + direction[1]]\n",
    "\n",
    "    def move(self):\n",
    "        while self.current_state not in environment.terminals:\n",
    "            action = self.policy[self.current_state]\n",
    "            self.current_state = self.transition(self.current_state, action)\n",
    "            self.path.append(self.current_state)\n",
    "            print(f\"Moved to: {self.current_state}\")\n",
    "        print(\"Terminal reached!\")\n",
    "\n",
    "    #def move(self):\n",
    "    #    while self.current_state not in self.environment.terminals:  # Check if current state is a terminal state\n",
    "    #        action = self.policy[self.current_state]\n",
    "    #        transitions = environment.transitions[self.current_state][action]\n",
    "    #        #curr_action = random.choices(transitions, weights=)\n",
    "    #        new_state = self.transition(self.current_state, action)\n",
    "    #        self.path.append(new_state)\n",
    "    #        self.current_state = new_state\n",
    "    #        #self.reward = environment.reward(self.current_state)\n",
    "    #        print(f\"Moved to: {self.current_state}\")\n",
    "#\n",
    "    #    print(\"Terminal reached!\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Test environment from your setup\n",
    "    REWARD = 20\n",
    "    environment = GridMDP([[REWARD, REWARD, REWARD, REWARD],\n",
    "                            [REWARD, None, REWARD, REWARD],\n",
    "                            [REWARD, REWARD, REWARD, REWARD],\n",
    "                            [REWARD, -1, REWARD, 10]],\n",
    "                            terminals=[(1, 0), (3, 0)])\n",
    "\n",
    "    # Solve the MDP with value iteration to get the best policy\n",
    "    pi = best_policy(environment, value_iteration(environment, 0.001))\n",
    "\n",
    "    # Create the vacuum agent using the best policy\n",
    "    vacuum_agent = MDPVacuumAgent(environment, pi)\n",
    "    vacuum_agent.move()\n",
    "\n",
    "    # Log the path followed\n",
    "    print(\"Path followed by the vacuum agent:\", vacuum_agent.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = best_policy(environment4, value_iteration(environment4, 0.001))\n",
    "\n",
    "def indexOf(state, direction): # applies direction to current_index\n",
    "    return [sum(i) for i in zip(state, direction)]\n",
    "\n",
    "start_state = (3,3)\n",
    "end_states = [(3,0),(1,0)]\n",
    "current_state = start_state\n",
    "\n",
    "while current_state not in end_states:\n",
    "    current_state = indexOf(current_state, policy[current_state])\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_instru(mdp, iterations=20):\n",
    "    U_over_time = []\n",
    "    U1 = {s: 0 for s in mdp.states}\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    for _ in range(iterations):\n",
    "        U = U1.copy()\n",
    "        for s in mdp.states:\n",
    "            U1[s] = R(s) + gamma * max([sum([p * U[s1] for (p, s1) in T(s, a)])\n",
    "                                        for a in mdp.actions(s)])\n",
    "        U_over_time.append(U)\n",
    "    return U_over_time\n",
    "\n",
    "columns = 4\n",
    "rows = 4\n",
    "U_over_time = value_iteration_instru(environment2)\n",
    "\n",
    "%matplotlib inline\n",
    "from notebook import make_plot_grid_step_function\n",
    "\n",
    "plot_grid_step = make_plot_grid_step_function(columns, rows, U_over_time)\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from notebook import make_visualize\n",
    "\n",
    "iteration_slider = widgets.IntSlider(min=1, max=15, step=1, value=0)\n",
    "w=widgets.interactive(plot_grid_step,iteration=iteration_slider)\n",
    "display(w)\n",
    "\n",
    "visualize_callback = make_visualize(iteration_slider)\n",
    "\n",
    "visualize_button = widgets.ToggleButton(description = \"Visualize\", value = False)\n",
    "time_select = widgets.ToggleButtons(description='Extra Delay:',options=['0', '0.1', '0.2', '0.5', '0.7', '1.0'])\n",
    "a = widgets.interactive(visualize_callback, Visualize = visualize_button, time_step=time_select)\n",
    "display(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
